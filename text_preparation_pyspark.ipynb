{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "For this text preparation process we are going to use the **PySpark** library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, rand, regexp_replace\n",
    "from sparknlp.annotator import Stemmer, LemmatizerModel\n",
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/14 19:49:43 WARN Utils: Your hostname, Cavelez resolves to a loopback address: 127.0.1.1; using 172.19.58.130 instead (on interface eth0)\n",
      "23/09/14 19:49:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/14 19:49:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|            hashtags|\n",
      "+--------------------+--------------------+\n",
      "|2020 is the year ...|#votethemout #cli...|\n",
      "|Winter has not st...|#climatefriday #c...|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|\n",
      "|A year of resista...|#greta #gretathun...|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|\n",
      "|10 Questions to A...|#climatechange #n...|\n",
      "|#climatestrike #F...|#climatestrike #F...|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|\n",
      "|My oldest daughte...|#climatestrike #l...|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|\n",
      "|\"\"\"The change is ...|#ClimateChange #c...|\n",
      "|Moments after #Im...|#ImpeachmentVote ...|\n",
      "|#climatestrike #C...|#climatestrike #C...|\n",
      "|Keep up the great...|#ClimateChangeIsR...|\n",
      "|Congratulations @...|#climatestrike #F...|\n",
      "|Even though I hop...|#HongKongProteste...|\n",
      "|*gretathunberg Is...|#PersonoftheYear ...|\n",
      "| Congratulations ...|#vegan #climatest...|\n",
      "|I get my energy a...|#ClimateStrike #F...|\n",
      "| THE CHAMBER OF C...|#greta #gretathun...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_in = \"twitterClimateData.csv\"\n",
    "df = spark.read.csv(path_in,inferSchema=True,header=True,sep=';')\n",
    "df = df.select([\"text\",\"hashtags\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72405"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preparation process\n",
    "\n",
    "The goal of this process is to reduce the number of tokens but without eliminating the intepretability of the words, in order to create the best bag of words possible. We are going to split this process for each column of the DataFrame, first for `text` column and then for `hashtags` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation process for `Text` Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='text',outputCol='text_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens=tokenization.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|            hashtags|         text_tokens|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|2020 is the year ...|#votethemout #cli...|[2020, is, the, y...|\n",
      "|Winter has not st...|#climatefriday #c...|[winter, has, not...|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|[week, 55, of, #c...|\n",
      "|A year of resista...|#greta #gretathun...|[a, year, of, res...|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|[, happy, holiday...|\n",
      "|10 Questions to A...|#climatechange #n...|[10, questions, t...|\n",
      "|#climatestrike #F...|#climatestrike #F...|[#climatestrike, ...|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|[#climatechangeis...|\n",
      "|My oldest daughte...|#climatestrike #l...|[my, oldest, daug...|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|[our, toddler, #p...|\n",
      "|\"\"\"The change is ...|#ClimateChange #c...|[\"\"\"the, change, ...|\n",
      "|Moments after #Im...|#ImpeachmentVote ...|[moments, after, ...|\n",
      "|#climatestrike #C...|#climatestrike #C...|[#climatestrike, ...|\n",
      "|Keep up the great...|#ClimateChangeIsR...|[keep, up, the, g...|\n",
      "|Congratulations @...|#climatestrike #F...|[congratulations,...|\n",
      "|Even though I hop...|#HongKongProteste...|[even, though, i,...|\n",
      "|*gretathunberg Is...|#PersonoftheYear ...|[*gretathunberg, ...|\n",
      "| Congratulations ...|#vegan #climatest...|[, congratulation...|\n",
      "|I get my energy a...|#ClimateStrike #F...|[i, get, my, ener...|\n",
      "| THE CHAMBER OF C...|#greta #gretathun...|[, the, chamber, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='text_tokens',outputCol='refined_text_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            hashtags|         text_tokens| refined_text_tokens|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|2020 is the year ...|#votethemout #cli...|[2020, is, the, y...|[2020, year, #vot...|\n",
      "|Winter has not st...|#climatefriday #c...|[winter, has, not...|[winter, stopped,...|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|[week, 55, of, #c...|[week, 55, #clima...|\n",
      "|A year of resista...|#greta #gretathun...|[a, year, of, res...|[year, resistance...|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|[, happy, holiday...|[, happy, holiday...|\n",
      "|10 Questions to A...|#climatechange #n...|[10, questions, t...|[10, questions, a...|\n",
      "|#climatestrike #F...|#climatestrike #F...|[#climatestrike, ...|[#climatestrike, ...|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|[#climatechangeis...|[#climatechangeis...|\n",
      "|My oldest daughte...|#climatestrike #l...|[my, oldest, daug...|[oldest, daughter...|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|[our, toddler, #p...|[toddler, #potus,...|\n",
      "|\"\"\"The change is ...|#ClimateChange #c...|[\"\"\"the, change, ...|[\"\"\"the, change, ...|\n",
      "|Moments after #Im...|#ImpeachmentVote ...|[moments, after, ...|[moments, #impeac...|\n",
      "|#climatestrike #C...|#climatestrike #C...|[#climatestrike, ...|[#climatestrike, ...|\n",
      "|Keep up the great...|#ClimateChangeIsR...|[keep, up, the, g...|[keep, great, wor...|\n",
      "|Congratulations @...|#climatestrike #F...|[congratulations,...|[congratulations,...|\n",
      "|Even though I hop...|#HongKongProteste...|[even, though, i,...|[even, though, ho...|\n",
      "|*gretathunberg Is...|#PersonoftheYear ...|[*gretathunberg, ...|[*gretathunberg, ...|\n",
      "| Congratulations ...|#vegan #climatest...|[, congratulation...|[, congratulation...|\n",
      "|I get my energy a...|#ClimateStrike #F...|[i, get, my, ener...|[get, energy, hop...|\n",
      "| THE CHAMBER OF C...|#greta #gretathun...|[, the, chamber, ...|[, chamber, comme...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_text_df=stopword_removal.transform(df_tokens)\n",
    "refined_text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------------+\n",
      "|                text|            hashtags|         text_tokens| refined_text_tokens|token_text_count|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------------+\n",
      "|2020 is the year ...|#votethemout #cli...|[2020, is, the, y...|[2020, year, #vot...|              21|\n",
      "|Winter has not st...|#climatefriday #c...|[winter, has, not...|[winter, stopped,...|              11|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|[week, 55, of, #c...|[week, 55, #clima...|              32|\n",
      "|A year of resista...|#greta #gretathun...|[a, year, of, res...|[year, resistance...|              25|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|[, happy, holiday...|[, happy, holiday...|              23|\n",
      "|10 Questions to A...|#climatechange #n...|[10, questions, t...|[10, questions, a...|              21|\n",
      "|#climatestrike #F...|#climatestrike #F...|[#climatestrike, ...|[#climatestrike, ...|              15|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|[#climatechangeis...|[#climatechangeis...|               3|\n",
      "|My oldest daughte...|#climatestrike #l...|[my, oldest, daug...|[oldest, daughter...|              15|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|[our, toddler, #p...|[toddler, #potus,...|              19|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType()) \n",
    "\n",
    "refined_text_df = refined_text_df.withColumn(\"token_text_count\", len_udf(col('refined_text_tokens')))\n",
    "\n",
    "refined_text_df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark NLP Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/14 19:50:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Trabajo2_almdatos/text_preparation_pyspark.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/camilo/Trabajo2_almdatos/text_preparation_pyspark.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Stemming\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/camilo/Trabajo2_almdatos/text_preparation_pyspark.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m stemmer \u001b[39m=\u001b[39m Stemmer()\u001b[39m.\u001b[39msetInputCols([\u001b[39m\"\u001b[39m\u001b[39mrefined_text_tokens\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39msetOutputCol(\u001b[39m\"\u001b[39m\u001b[39mstemming_text_tokens\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Trabajo2_almdatos-PXMnXu75/lib/python3.11/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMethod \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m forces keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_kwargs \u001b[39m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Trabajo2_almdatos-PXMnXu75/lib/python3.11/site-packages/sparknlp/annotator/stemmer.py:76\u001b[0m, in \u001b[0;36mStemmer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m@keyword_only\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     \u001b[39msuper\u001b[39;49m(Stemmer, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(classname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcom.johnsnowlabs.nlp.annotators.Stemmer\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setDefault(\n\u001b[1;32m     78\u001b[0m         language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Trabajo2_almdatos-PXMnXu75/lib/python3.11/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMethod \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m forces keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_kwargs \u001b[39m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Trabajo2_almdatos-PXMnXu75/lib/python3.11/site-packages/sparknlp/common/annotator_model.py:37\u001b[0m, in \u001b[0;36mAnnotatorModel.__init__\u001b[0;34m(self, classname, java_model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m classname \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m java_model:\n\u001b[1;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m_java_class_name \u001b[39m=\u001b[39m classname\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_java_obj(classname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muid)\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m java_model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_from_java()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Trabajo2_almdatos-PXMnXu75/lib/python3.11/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m java_obj(\u001b[39m*\u001b[39;49mjava_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "stemmer = Stemmer().setInputCols([\"refined_text_tokens\"]).setOutputCol(\"stemming_text_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['refined_text_tokens']) \\\n",
    "     .setOutputCol('lemmatize_text_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation process for `Hashtags` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|            hashtags|   hashtags_without#|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|2020 is the year ...|#votethemout #cli...|votethemout clima...|\n",
      "|Winter has not st...|#climatefriday #c...|climatefriday cli...|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|       ClimateStrike|\n",
      "|A year of resista...|#greta #gretathun...|greta gretathunbe...|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|greta gretathunbe...|\n",
      "|10 Questions to A...|#climatechange #n...|climatechange nat...|\n",
      "|#climatestrike #F...|#climatestrike #F...|climatestrike Fri...|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|ClimateChangeIsRe...|\n",
      "|My oldest daughte...|#climatestrike #l...|climatestrike let...|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|POTUS Time GretaT...|\n",
      "|\"\"\"The change is ...|#ClimateChange #c...|ClimateChange cli...|\n",
      "|Moments after #Im...|#ImpeachmentVote ...|ImpeachmentVote C...|\n",
      "|#climatestrike #C...|#climatestrike #C...|climatestrike Cli...|\n",
      "|Keep up the great...|#ClimateChangeIsR...|ClimateChangeIsRe...|\n",
      "|Congratulations @...|#climatestrike #F...|climatestrike Fri...|\n",
      "|Even though I hop...|#HongKongProteste...|HongKongProtester...|\n",
      "|*gretathunberg Is...|#PersonoftheYear ...|PersonoftheYear H...|\n",
      "| Congratulations ...|#vegan #climatest...| vegan climatestrike|\n",
      "|I get my energy a...|#ClimateStrike #F...|ClimateStrike Fir...|\n",
      "| THE CHAMBER OF C...|#greta #gretathun...|greta gretathunbe...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"hashtags_without#\",regexp_replace(\"hashtags\",\"#\",\"\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='hashtags_without#',outputCol='hashtags_tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags=tokenization.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            hashtags|   hashtags_without#|     hashtags_tokens|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|2020 is the year ...|#votethemout #cli...|votethemout clima...|[votethemout, cli...|\n",
      "|Winter has not st...|#climatefriday #c...|climatefriday cli...|[climatefriday, c...|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|       ClimateStrike|     [climatestrike]|\n",
      "|A year of resista...|#greta #gretathun...|greta gretathunbe...|[greta, gretathun...|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|greta gretathunbe...|[greta, gretathun...|\n",
      "|10 Questions to A...|#climatechange #n...|climatechange nat...|[climatechange, n...|\n",
      "|#climatestrike #F...|#climatestrike #F...|climatestrike Fri...|[climatestrike, f...|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|ClimateChangeIsRe...|[climatechangeisr...|\n",
      "|My oldest daughte...|#climatestrike #l...|climatestrike let...|[climatestrike, l...|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|POTUS Time GretaT...|[potus, time, gre...|\n",
      "|\"\"\"The change is ...|#ClimateChange #c...|ClimateChange cli...|[climatechange, c...|\n",
      "|Moments after #Im...|#ImpeachmentVote ...|ImpeachmentVote C...|[impeachmentvote,...|\n",
      "|#climatestrike #C...|#climatestrike #C...|climatestrike Cli...|[climatestrike, c...|\n",
      "|Keep up the great...|#ClimateChangeIsR...|ClimateChangeIsRe...|[climatechangeisr...|\n",
      "|Congratulations @...|#climatestrike #F...|climatestrike Fri...|[climatestrike, f...|\n",
      "|Even though I hop...|#HongKongProteste...|HongKongProtester...|[hongkongproteste...|\n",
      "|*gretathunberg Is...|#PersonoftheYear ...|PersonoftheYear H...|[personoftheyear,...|\n",
      "| Congratulations ...|#vegan #climatest...| vegan climatestrike|[vegan, climatest...|\n",
      "|I get my energy a...|#ClimateStrike #F...|ClimateStrike Fir...|[climatestrike, f...|\n",
      "| THE CHAMBER OF C...|#greta #gretathun...|greta gretathunbe...|[greta, gretathun...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hashtags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='hashtags_tokens',outputCol='refined_hashtags_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----------------------+\n",
      "|                text|            hashtags|   hashtags_without#|     hashtags_tokens|refined_hashtags_tokens|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------------+\n",
      "|2020 is the year ...|#votethemout #cli...|votethemout clima...|[votethemout, cli...|   [votethemout, cli...|\n",
      "|Winter has not st...|#climatefriday #c...|climatefriday cli...|[climatefriday, c...|   [climatefriday, c...|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|       ClimateStrike|     [climatestrike]|        [climatestrike]|\n",
      "|A year of resista...|#greta #gretathun...|greta gretathunbe...|[greta, gretathun...|   [greta, gretathun...|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|greta gretathunbe...|[greta, gretathun...|   [greta, gretathun...|\n",
      "|10 Questions to A...|#climatechange #n...|climatechange nat...|[climatechange, n...|   [climatechange, n...|\n",
      "|#climatestrike #F...|#climatestrike #F...|climatestrike Fri...|[climatestrike, f...|   [climatestrike, f...|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|ClimateChangeIsRe...|[climatechangeisr...|   [climatechangeisr...|\n",
      "|My oldest daughte...|#climatestrike #l...|climatestrike let...|[climatestrike, l...|   [climatestrike, l...|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|POTUS Time GretaT...|[potus, time, gre...|   [potus, time, gre...|\n",
      "|\"\"\"The change is ...|#ClimateChange #c...|ClimateChange cli...|[climatechange, c...|   [climatechange, c...|\n",
      "|Moments after #Im...|#ImpeachmentVote ...|ImpeachmentVote C...|[impeachmentvote,...|   [impeachmentvote,...|\n",
      "|#climatestrike #C...|#climatestrike #C...|climatestrike Cli...|[climatestrike, c...|   [climatestrike, c...|\n",
      "|Keep up the great...|#ClimateChangeIsR...|ClimateChangeIsRe...|[climatechangeisr...|   [climatechangeisr...|\n",
      "|Congratulations @...|#climatestrike #F...|climatestrike Fri...|[climatestrike, f...|   [climatestrike, f...|\n",
      "|Even though I hop...|#HongKongProteste...|HongKongProtester...|[hongkongproteste...|   [hongkongproteste...|\n",
      "|*gretathunberg Is...|#PersonoftheYear ...|PersonoftheYear H...|[personoftheyear,...|   [personoftheyear,...|\n",
      "| Congratulations ...|#vegan #climatest...| vegan climatestrike|[vegan, climatest...|   [vegan, climatest...|\n",
      "|I get my energy a...|#ClimateStrike #F...|ClimateStrike Fir...|[climatestrike, f...|   [climatestrike, f...|\n",
      "| THE CHAMBER OF C...|#greta #gretathun...|greta gretathunbe...|[greta, gretathun...|   [greta, gretathun...|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_hashtags_df=stopword_removal.transform(df_hashtags)\n",
    "refined_hashtags_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+\n",
      "|                text|            hashtags|   hashtags_without#|     hashtags_tokens|refined_hashtags_tokens|token_hashtags_count|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+\n",
      "|2020 is the year ...|#votethemout #cli...|votethemout clima...|[votethemout, cli...|   [votethemout, cli...|                   3|\n",
      "|Winter has not st...|#climatefriday #c...|climatefriday cli...|[climatefriday, c...|   [climatefriday, c...|                   3|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|       ClimateStrike|     [climatestrike]|        [climatestrike]|                   1|\n",
      "|A year of resista...|#greta #gretathun...|greta gretathunbe...|[greta, gretathun...|   [greta, gretathun...|                  11|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|greta gretathunbe...|[greta, gretathun...|   [greta, gretathun...|                  15|\n",
      "|10 Questions to A...|#climatechange #n...|climatechange nat...|[climatechange, n...|   [climatechange, n...|                  14|\n",
      "|#climatestrike #F...|#climatestrike #F...|climatestrike Fri...|[climatestrike, f...|   [climatestrike, f...|                  13|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|ClimateChangeIsRe...|[climatechangeisr...|   [climatechangeisr...|                   3|\n",
      "|My oldest daughte...|#climatestrike #l...|climatestrike let...|[climatestrike, l...|   [climatestrike, l...|                   3|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|POTUS Time GretaT...|[potus, time, gre...|   [potus, time, gre...|                  11|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType()) \n",
    "\n",
    "refined_hashtags_df = refined_hashtags_df.withColumn(\"token_hashtags_count\", len_udf(col('refined_hashtags_tokens')))\n",
    "\n",
    "refined_hashtags_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1) for stemming and lemmatizing \n",
    "* https://www.johnsnowlabs.com/boost-your-nlp-results-with-spark-nlp-stemming-and-lemmatizing-techniques/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trabajo2_almdatos-PXMnXu75",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
