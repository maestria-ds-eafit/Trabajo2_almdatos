{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "For this text preparation process we are going to use the **PySpark** library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import Stemmer, LemmatizerModel, Tokenizer, Normalizer, StopWordsCleaner\n",
    "from sparknlp.base import DocumentAssembler, Pipeline, Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/14 21:18:21 WARN Utils: Your hostname, DESKTOP-KCSPFSJ resolves to a loopback address: 127.0.1.1; using 172.24.244.59 instead (on interface eth0)\n",
      "23/09/14 21:18:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/david/.local/share/virtualenvs/Trabajo2_almdatos-LZAGjvTA/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/david/.ivy2/cache\n",
      "The jars for the packages stored in: /home/david/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-41541c31-a09e-476b-bd6d-79d614a4916e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.1.1 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      ":: resolution report :: resolve 1391ms :: artifacts dl 33ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.1.1 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   75  |   0   |   0   |   3   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-41541c31-a09e-476b-bd6d-79d614a4916e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/12ms)\n",
      "23/09/14 21:18:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======>                                                    (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|            hashtags|\n",
      "+--------------------+--------------------+\n",
      "|2020 is the year ...|#votethemout #cli...|\n",
      "|Winter has not st...|#climatefriday #c...|\n",
      "|WEEK 55 of #Clima...|      #ClimateStrike|\n",
      "|A year of resista...|#greta #gretathun...|\n",
      "| HAPPY HOLIDAYS #...|#greta #gretathun...|\n",
      "|10 Questions to A...|#climatechange #n...|\n",
      "|#climatestrike #F...|#climatestrike #F...|\n",
      "|#ClimateChangeIsR...|#ClimateChangeIsR...|\n",
      "|My oldest daughte...|#climatestrike #l...|\n",
      "|Our toddler #POTU...|#POTUS #Time #Gre...|\n",
      "|\"\"\"The change is ...|#ClimateChange #c...|\n",
      "|Moments after #Im...|#ImpeachmentVote ...|\n",
      "|#climatestrike #C...|#climatestrike #C...|\n",
      "|Keep up the great...|#ClimateChangeIsR...|\n",
      "|Congratulations @...|#climatestrike #F...|\n",
      "|Even though I hop...|#HongKongProteste...|\n",
      "|*gretathunberg Is...|#PersonoftheYear ...|\n",
      "| Congratulations ...|#vegan #climatest...|\n",
      "|I get my energy a...|#ClimateStrike #F...|\n",
      "| THE CHAMBER OF C...|#greta #gretathun...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path_in = \"twitterClimateData.csv\"\n",
    "df = spark.read.csv(path_in,inferSchema=True,header=True,sep=';')\n",
    "df = df.select([\"text\",\"hashtags\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72405"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preparation process\n",
    "\n",
    "The goal of this process is to reduce the number of tokens but without eliminating the intepretability of the words, in order to create the best bag of words possible. We are going to split this process for each column of the DataFrame, first for `text` column and then for `hashtags` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation process for `Text` Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = (\n",
    "    DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document_text\")\n",
    ")\n",
    "tokenizer = Tokenizer().setInputCols([\"document_text\"]).setOutputCol(\"text_tokens\")\n",
    "normalizer = Normalizer().setInputCols([\"text_tokens\"]).setOutputCol(\"text_normalized\")\n",
    "stop_word_remover = (\n",
    "    StopWordsCleaner().setInputCols([\"text_normalized\"]).setOutputCol(\"text_stop_words_cleaned\")\n",
    ")\n",
    "stemmer = Stemmer().setInputCols([\"text_stop_words_cleaned\"]).setOutputCol(\"text_stemmed\")\n",
    "lemmatizer = (\n",
    "    LemmatizerModel.pretrained()\n",
    "    .setInputCols([\"text_stemmed\"])\n",
    "    .setOutputCol(\"text_lemmatized\")\n",
    ")\n",
    "finisher = Finisher().setInputCols([\"text_lemmatized\"]).setOutputCols([\"refined_text\"]).setOutputAsArray(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/david/.local/share/virtualenvs/Trabajo2_almdatos-LZAGjvTA/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.4.1.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|        refined_text|\n",
      "+--------------------+--------------------+\n",
      "|2020 is the year ...|[year, votethemou...|\n",
      "|Winter has not st...|[winter, stop, gr...|\n",
      "|WEEK 55 of #Clima...|[week, climatestr...|\n",
      "|A year of resista...|[year, resist, yo...|\n",
      "| HAPPY HOLIDAYS #...|[happi, holidai, ...|\n",
      "|10 Questions to A...|[question, ask, p...|\n",
      "|#climatestrike #F...|[climatestrik, fr...|\n",
      "|#ClimateChangeIsR...|[climatechangeisr...|\n",
      "|My oldest daughte...|[old, daughter, f...|\n",
      "|Our toddler #POTU...|[toddler, potu, w...|\n",
      "|\"\"\"The change is ...|[chang, go, come,...|\n",
      "|Moments after #Im...|[moment, impeachm...|\n",
      "|#climatestrike #C...|[climatestrik, cl...|\n",
      "|Keep up the great...|[keep, great, wor...|\n",
      "|Congratulations @...|[congratul, greta...|\n",
      "|Even though I hop...|[even, though, ho...|\n",
      "|*gretathunberg Is...|[gretathunberg, y...|\n",
      "| Congratulations ...|[congratul, inspi...|\n",
      "|I get my energy a...|[get, energi, hop...|\n",
      "| THE CHAMBER OF C...|[chamber, commerc...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline().setStages(\n",
    "    [documentAssembler, tokenizer, normalizer, stop_word_remover, stemmer, lemmatizer, finisher]\n",
    ")\n",
    "transformed_df = pipeline.fit(df).transform(df)\n",
    "transformed_df.select([\"text\", \"refined_text\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Spark NLP Stemming and Lemmatizing - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Text preparation process for `Hashtags` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = (\n",
    "    DocumentAssembler().setInputCol(\"hashtags\").setOutputCol(\"document_hashtags\")\n",
    ")\n",
    "tokenizer = Tokenizer().setInputCols([\"document_hashtags\"]).setOutputCol(\"hashtags_tokens\")\n",
    "normalizer = Normalizer().setInputCols([\"hashtags_tokens\"]).setOutputCol(\"hashtags_normalized\")\n",
    "stop_word_remover = (\n",
    "    StopWordsCleaner().setInputCols([\"hashtags_normalized\"]).setOutputCol(\"hashtags_stop_words_cleaned\")\n",
    ")\n",
    "stemmer = Stemmer().setInputCols([\"hashtags_stop_words_cleaned\"]).setOutputCol(\"hashtags_stemmed\")\n",
    "lemmatizer = (\n",
    "    LemmatizerModel.pretrained()\n",
    "    .setInputCols([\"hashtags_stemmed\"])\n",
    "    .setOutputCol(\"hashtags_lemmatized\")\n",
    ")\n",
    "finisher = Finisher().setInputCols([\"hashtags_lemmatized\"]).setOutputCols([\"refined_hashtags\"]).setOutputAsArray(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            hashtags|    refined_hashtags|\n",
      "+--------------------+--------------------+\n",
      "|#votethemout #cli...|[votethemout, cli...|\n",
      "|#climatefriday #c...|[climatefridai, c...|\n",
      "|      #ClimateStrike|      [climatestrik]|\n",
      "|#greta #gretathun...|[greta, gretathun...|\n",
      "|#greta #gretathun...|[greta, gretathun...|\n",
      "|#climatechange #n...|[climatechang, na...|\n",
      "|#climatestrike #F...|[climatestrik, fr...|\n",
      "|#ClimateChangeIsR...|[climatechangeisr...|\n",
      "|#climatestrike #l...|[climatestrik, le...|\n",
      "|#POTUS #Time #Gre...|[potu, time, gret...|\n",
      "|#ClimateChange #c...|[climatechang, cl...|\n",
      "|#ImpeachmentVote ...|[impeachmentvot, ...|\n",
      "|#climatestrike #C...|[climatestrik, cl...|\n",
      "|#ClimateChangeIsR...|[climatechangeisr...|\n",
      "|#climatestrike #F...|[climatestrik, fr...|\n",
      "|#HongKongProteste...|[hongkongprotest,...|\n",
      "|#PersonoftheYear ...|[personoftheyear,...|\n",
      "|#vegan #climatest...|[vegan, climatest...|\n",
      "|#ClimateStrike #F...|[climatestrik, fi...|\n",
      "|#greta #gretathun...|[greta, gretathun...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline().setStages(\n",
    "    [documentAssembler, tokenizer, normalizer, stop_word_remover, stemmer, lemmatizer, finisher]\n",
    ")\n",
    "transformed_df = pipeline.fit(df).transform(df)\n",
    "transformed_df.select([\"hashtags\", \"refined_hashtags\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Spark NLP Stemming and Lemmatizing - Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1) for stemming and lemmatizing \n",
    "* https://medium.com/trustyou-engineering/topic-modelling-with-pyspark-and-spark-nlp-a99d063f1a6e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trabajo2_almdatos-PXMnXu75",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
